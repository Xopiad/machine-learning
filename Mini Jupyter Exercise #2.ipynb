{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <h1>ECE 445: Machine Learning for Engineers - Albert Tran</h1>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <h2>Mini Jupyter Exercise #2</h2>\n",
    "    <p><strong>Objective</strong>: The goal of this exercise is to explore various aspects of principal components analysis (PCA)\n",
    "using both synthetic data and real data.</p>\n",
    "    <h3>Synthetic Data</h3>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[ 0.20586487  1.16676171]\n",
      " [-2.07263978 -0.63268717]\n",
      " [ 0.99712645  2.3945538 ]]\n",
      "\n",
      "Rank of Matrix A:  2\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy.linalg import matrix_rank\n",
    "\n",
    "np.random.seed(53) #Same random numbers every time\n",
    "A = np.random.normal(0,1,(3,2)) #Generate random 3x2 matrix with normal random entries\n",
    "print(\"A:\", A)\n",
    "print(\"\\nRank of Matrix A: \", matrix_rank(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <h4>Generation of Dataset #1</h4>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of Data Matrix X: (3, 500)\n",
      "Dimensionality of Data Sample X[i]: (3, 1)\n",
      "Rank of X: 2\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((500,3,1))\n",
    "np.random.seed(53)\n",
    "for i in range(500):\n",
    "    x[i] = np.matrix(A)*np.matrix(np.random.normal(0,1,(2,1))) #x=Av\n",
    "x = np.matrix(np.transpose(x))\n",
    "x.shape = (3,500)\n",
    "\n",
    "print(\"Dimensionality of Data Matrix X:\", x.shape)\n",
    "print(\"Dimensionality of Data Sample X[i]:\", x[:,0].shape)\n",
    "print(\"Rank of X:\", matrix_rank(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <h4>Singular Value and Eigenvalue Decomposition of Dataset #1</h4>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Singular Vectors of X:\n",
      " [[-0.32929221 -0.35219682 -0.87608449]\n",
      " [ 0.53273339 -0.83535147  0.13558416]\n",
      " [-0.77959078 -0.42207265  0.46270174]]\n",
      "Eigenvectors of X*X^T:\n",
      " [[-0.32929221  0.87608449  0.35219682]\n",
      " [ 0.53273339 -0.13558416  0.83535147]\n",
      " [-0.77959078 -0.46270174  0.42207265]]\n"
     ]
    }
   ],
   "source": [
    "x_u, x_sigma, x_v = np.linalg.svd(x, full_matrices = True)#X=U(SIGMA)V\n",
    "x_lambda, x_q = np.linalg.eig(x*np.matrix(np.transpose(x)))#X*X^T=Q(LAMBDA)Q^-1\n",
    "print(\"Left Singular Vectors of X:\\n\",x_u)\n",
    "print(\"Eigenvectors of X*X^T:\\n\",x_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left singular vectors of X correspond to the eigenvectors of X*X^T, although some of the vectors have been reflected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared Singluar Values of X: [5.53312162e+03 1.26668331e+03 8.09051082e-30]\n",
      "Eigenvalues of X*X^T: [5.53312162e+03 2.54449239e-13 1.26668331e+03]\n"
     ]
    }
   ],
   "source": [
    "print(\"Squared Singluar Values of X:\",np.square(x_sigma))\n",
    "print(\"Eigenvalues of X*X^T:\",x_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The squared singular values of X are the same as the eigenvalues of X*X^T, since the 2.54449239e-13 is very small but due to rounding is not zero, the corresponding squared singular value is even smaller but not zero as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square of the Frobenius Norm of X: 6799.804931975367\n",
      "Sum of Squared Singular Values of X: 6799.80493197537\n"
     ]
    }
   ],
   "source": [
    "print(\"Square of the Frobenius Norm of X:\",np.square(np.linalg.norm(x)))\n",
    "print(\"Sum of Squared Singular Values of X:\",sum(np.square(x_sigma)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The energy in X equal to the sum of squared singular values of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While none of the singluar values of X are zero, one of them is very small compared to the other singular values which implies that is should be zero, the reason it is not zero is because of the rounding error in python. Another reason is that SVD has an error since it is a low-rank approximation of X.\n",
    "\n",
    "# The relationship between the left singular values of X that correspond to the 2 largest singular values and the columns of A is that they are linear combinations of each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <h4>PCA of Dataset #1</h4>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed before, the rank of X is 2. This is because of the following theorem: <br><br>\n",
    "$rank(AV)â‰¤min(rank(A),rank(V))$ <br><br>\n",
    "Therefore since the rank of A is 2 and the rank of V is also 2, the rank of X is 2. This means in the data matrix X (3x500), where each data sample is a column of X, one of the 3 rows is linearly dependent on the other two. This means PCA can reduce the number of rows needed from 3 to 2, since that information can be expressed by the other two rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the values of the matrix A and the matrix V were generated from a normal distrbution with an expected value of 0, X is a bivariate normal distribution with an expected value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Vector of X: [[-0.05410283]\n",
      " [-0.02444551]\n",
      " [-0.09527569]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Vector of X:\", x.mean(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean vector of X is shown to be small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
